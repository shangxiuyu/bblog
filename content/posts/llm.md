---
title: "llm"
date: 2025-01-23
draft: false
tags: ["llm", "AI", "大模型认知"]
---

llm：大语言模型（large language model）

大型语言模型（LLM）基于一种称为Transformer的架构，旨在理解、生成和响应人类的语言文本，这些模型在大量文本数据上进行训练，包括来自公开可用资源的大量数据，如网络抓取、书籍和维基百科等

“大语言模型”中的“大”既指模型的参数大小，也指其训练所用的庞大数据集。这样的模型通常具有数十亿甚至数百亿个参数，这些参数是网络中在训练期间进行优化的可调节权重，以预测序列中的下一个单词(word)。下一个单词预测是有意义的，因为它利用语言的固有顺序性质来训练模型理解文本中的上下文、结构和关系。

在大模型中，token被视为模型处理和生成的基本单位，token可能是一个单词，可能是一个字或一个词语（取决于所使用的分词方法）

| 技术阶段 | 面向人群 | 技术积累 | 应用场景 | 特征总结 |
| --- | --- | --- | --- | --- |
| 提示工程 (Prompt Engineering) | 终端用户 | 对 ChatGPT 等模型应用的提示词有基础的了解和使用 | 文本生成、机器翻译等 | 门槛低，易于上手 |
| AI 智能体 (Agents) | 大模型应用开发人员 | 了解大模型基础原理和理论，熟悉相关任务领域的业务逻辑和流程 | AI客服、虚拟助理等 | 偏重于交互性和用户体验 |
| 大模型微调 (Fine-tuning) | 领域模型研发、私有化团队 | 掌握神经网络和机器学习概念，有数据处理和模型训练经验 | 语义理解、领域知识学习 | 通用性强，性价比高 |
| 预训练技术 (Pre-training) | 大模型研究人员、数据科学家 | 熟悉深度学习原理和网络架构，有大规模数据处理和模型训练经验 | 多模态学习、语言模型预训练 | 前期投入大但效果显著 |
|  |  |  |  |  |

## Transformer架构

大多数大语言模型都依赖于Transformer架构，这是一种深度神经网络架构。

Transformer架构被在2017年的论文[《Attention Is All You Need》](https://arxiv.org/abs/1706.03762) 中首次提出。

Transformer架构由两个子模块组成：编码器和解码器。[深度学习的核心架构与基本原理](https://waytoagi.feishu.cn/wiki/QnnKwxR0SihLKYk1EIKc1HXonnb)

- 编码器模块处理输入文本并将其编码成一系列数值表示或向量，这些向量捕获输入的上下文信息。（处理输入文本并生成文本的嵌入表示）
- 解码器模块获取这些编码向量并生成输出文本。
- 编码器和解码器都由许多层组成，这些层通过一种称为自注意力机制的方式连接。

Transformer的关键组成部分是自注意力机制，它允许模型对序列中不同单词或token的相对重要性进行加权。这种机制使模型能够捕获输入数据中的长距离依赖关系和上下文关系，从而增强其生成连贯且与上下文相关的输出的能力。

BERT（Bidirectional Encoder Representations from Transformers 的缩写）和各种 GPT 模型（Generative Pre-trained Transformers 的缩写）都是Transformer架构的变体。

## 构建一个LLM

### 预训练和微调

创建LLM的一般过程包括预训练和微调：

- “预训练”中的“预”是指初始阶段，其中像LLM这样的模型在大规模、多样化的数据集上进行训练
- “微调” - 预训练的模型作为一种基础资源，可以通过微调进一步完善，微调是一个过程，其中模型专门针对特定任务或领域的数据集进行训练

创建LLM的第一步是在大量文本数据语料库上对其进行训练，有时称为原始文本。这里的“原始”是指该数据只是常规文本，没有任何标记信息。预训练的LLM，通常被称为基础模型。

在从大型文本数据集上训练获得预训练的LLM后，可以进一步在标记数据上训练LLM，即微调。

预训练大型语言模型需要大量资源，成本非常高。好在许多预训练的大型语言模型，作为开源模型提供，以此为基础，可以通过在特定的任务或领域的数据集进行微调，进一步完善。

### 指令微调和分类微调

两个常用的微调种类：

- 指令微调：标记数据集由指令和答案对组成。例如翻译文本的输入以及译文。
- 分类微调：标记数据集由文本和关联的类标签组成。例如，与“垃圾邮件”和“非垃圾邮件”标签关联的电子邮件。
